{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 128 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import glob\n",
    "import json\n",
    "import pickle as pkl\n",
    "import ast\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from random import sample\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import glob\n",
    "import json\n",
    "import pickle as pkl\n",
    "import ast\n",
    "import os\n",
    "from random import sample\n",
    "import networkx as nx\n",
    "\n",
    "from random import sample\n",
    "import sklearn\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "import gensim\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEVEL1_FOLDERS = ['groups']\n",
    "allfiles = []\n",
    "\n",
    "# for l in LEVEL1_FOLDERS:\n",
    "#     l1path = '/data/shruti/ONR/big_data/facebook/' + l + '/*'\n",
    "#     LEVEL2_FOLDERS = glob.glob(l1path)\n",
    "#     for l2 in LEVEL2_FOLDERS:\n",
    "#         l2path = l2 + \"/*.json\"\n",
    "#         fnames = glob.glob(l2path)\n",
    "#         for f in fnames:\n",
    "#             if os.stat(f).st_size == 0:\n",
    "#                 fakevar=1\n",
    "#             else:\n",
    "#                 allfiles.append(f)\n",
    "\n",
    "allfiles = ['/data/shruti/ONR/big_data/facebook/groups/re_hindi/2021-07-01_2021-08-01.json']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(fileName):\n",
    "    dictlist = []\n",
    "    with open(fileName, \"r\") as jfile:\n",
    "        for line in jfile:\n",
    "            tempdict = {}\n",
    "            try:\n",
    "                job = json.loads(line)\n",
    "            except:\n",
    "                job = None\n",
    "            if job!=None:    \n",
    "                if \"date\" in job:\n",
    "                    tempdict['date'] = job['date']\n",
    "                    if \"title\" in job:\n",
    "                        tempdict['title'] = job['title']\n",
    "                    if \"caption\" in job:\n",
    "                        tempdict['caption'] = job['caption']\n",
    "                    if \"description\" in job:\n",
    "                        tempdict['description'] = job['description']\n",
    "                    if \"description\" in job:\n",
    "                        tempdict['description'] = job['description']\n",
    "                    if \"message\" in job:\n",
    "                        tempdict['message'] = job['message']\n",
    "                    if \"link\" in job:\n",
    "                        tempdict['link'] = job['link']\n",
    "                    if \"postUrl\" in job:\n",
    "                        tempdict['postUrl'] = job['postUrl']\n",
    "                    if \"score\" in job:\n",
    "                        tempdict['score'] = job['score']\n",
    "\n",
    "                    if \"account\" in job:\n",
    "                        try:\n",
    "                            tempdict['handle'] = job['account']['name']\n",
    "                        except:\n",
    "                            fakevar=1\n",
    "\n",
    "                        try:\n",
    "                            tempdict['platformId'] = job['account']['platformId']\n",
    "                        except:\n",
    "                            fakevar=1\n",
    "\n",
    "                        try:\n",
    "                            tempdict['id'] = job['account']['id']\n",
    "                        except:\n",
    "                            fakevar=1\n",
    "\n",
    "                    dictlist.append(tempdict)\n",
    "\n",
    "    if len(dictlist) > 0:\n",
    "        return dictlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filcontent = pd.DataFrame()\n",
    "filcontent['fileNames'] = allfiles\n",
    "filcontent['content'] = filcontent['fileNames'].parallel_apply(lambda x: get_contents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonList = []\n",
    "\n",
    "for idx, row in filcontent.iterrows():\n",
    "    for c in row['content']:\n",
    "        jsonList.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata = pd.DataFrame(jsonList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567297\n",
      "293716\n"
     ]
    }
   ],
   "source": [
    "print(len(fdata))\n",
    "fdata = fdata.dropna(subset=['message'])\n",
    "print(len(fdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293716"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdata = fdata.drop_duplicates()\n",
    "len(fdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263520"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdata = fdata.drop_duplicates(subset=['handle','message'])\n",
    "len(fdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "remove_urls = lambda x: re.sub(\"http(.+)?(\\W|$)\", ' ', x)\n",
    "remove_RT = lambda x: x.replace(\"RT \", \"\")\n",
    "remove_mentions = lambda x: re.sub(\"@\\S+\", '', x)\n",
    "remove_hashtags = lambda x: re.sub(\"#\\S+\", '', x)\n",
    "remove_digits = lambda x: re.sub(\"\\d+\", \"\", x)\n",
    "remove_punct = lambda x: re.sub(\"!|\\||\\%|\\.|\\-|\\/|:|…|,|\\?|।+|'|⁉|\\*|‘|’|\\\"|\\(|\\)+\", \"\", x)\n",
    "remove_emojis = lambda x: emoji_pattern.sub(\"\", x)\n",
    "normalize_spaces = lambda x: re.sub(\"[\\n\\r\\t ]+\", ' ', x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_noise = lambda x:normalize_spaces(\n",
    "                                remove_emojis(\n",
    "                                    remove_punct(\n",
    "                                        remove_digits(\n",
    "                                            remove_hashtags(\n",
    "                                                remove_mentions(\n",
    "                                                    remove_punct(\n",
    "                                                        remove_urls(x.lower()))))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata['clean_text'] = fdata['message'].parallel_apply(lambda x: remove_noise(x) if len(x) >15 else '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = fdata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263520\n",
      "255479\n"
     ]
    }
   ],
   "source": [
    "print(len(frame))\n",
    "frame = frame.loc[frame['clean_text']!='0']\n",
    "print(len(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['textlen'] = frame['clean_text'].parallel_apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137899"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = frame.loc[(frame['textlen']>14)&(frame['textlen']<240)]\n",
    "len(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136875\n"
     ]
    }
   ],
   "source": [
    "frame = frame.loc[frame['title']!='This is a re-share of a post']\n",
    "frame = frame.drop_duplicates(subset=['handle','message'])\n",
    "print(len(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frame = frame#.sample(100000)\n",
    "sample_text = sample_frame['clean_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "embeddings = model.encode(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136875, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 'july_'\n",
    "sframefile = \"/data/shruti/ONR/small_data/embeddings/\" + month + \"frame.csv\"\n",
    "sembedfile = \"/data/shruti/ONR/small_data/embeddings/\" + month + \"embeddings.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/shruti/ONR/small_data/embeddings/july_frame.csv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sframefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frame.to_csv(sframefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(sembedfile, \"wb\") as pfile:\n",
    "    pkl.dump(embeddings, pfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
